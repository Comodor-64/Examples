{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import holidays\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import catboost as cbt\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    root= 'D:\\Vs_Code\\enefit'\n",
    "    lc_cl = ['longitude', 'latitude', 'county']\n",
    "    #Weather Station Cordinates (lc)\n",
    "\n",
    "    tg_cl = ['target', 'county', 'is_business', 'product_type', 'is_consumption', \n",
    "             'datetime']\n",
    "    #Target Columns Name (tg) \n",
    "    \n",
    "    tr_cl = ['target', 'county', 'is_business', 'product_type', 'is_consumption',\n",
    "             'datetime' ,'row_id']\n",
    "    #Train Columns Name (tr)\n",
    "\n",
    "    cl_cl =  ['product_type', 'county', 'eic_count', 'installed_capacity',\n",
    "              'is_business','date']\n",
    "    #Client Columns Name (cl)\n",
    "\n",
    "    gs_cl = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
    "    #Gas Prices Columns Name (gs)\n",
    "\n",
    "    ec_cl = ['forecast_date', 'euros_per_mwh']\n",
    "    #Electric Price Columns Name (ec)\n",
    "\n",
    "    fr_cl = ['latitude', 'longitude', 'hours_ahead', 'temperature',\n",
    "             'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid',\n",
    "             'cloudcover_total', '10_metre_u_wind_component',\n",
    "             '10_metre_v_wind_component', 'forecast_datetime',\n",
    "             'direct_solar_radiation', 'surface_solar_radiation_downwards',\n",
    "             'snowfall', 'total_precipitation']\n",
    "    #Forecast Weather Columns Name (fr)\n",
    "\n",
    "    hs_cl = ['datetime','temperature','dewpoint','rain','snowfall',\n",
    "             'surface_pressure','cloudcover_total','cloudcover_low',\n",
    "             'cloudcover_mid','cloudcover_high','windspeed_10m',\n",
    "             'winddirection_10m','shortwave_radiation','direct_solar_radiation',\n",
    "             'diffuse_radiation','latitude','longitude']\n",
    "    #Historical Weather Columns Name (hs)\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.df_tr=pl.read_csv(os.path.join(self.root, 'train.csv'),\n",
    "                               columns=self.tr_cl, try_parse_dates=True)\n",
    "\n",
    "        self.df_cl=pl.read_csv(os.path.join(self.root, 'client.csv')\n",
    "                               ,columns=self.cl_cl, try_parse_dates=True)\n",
    "\n",
    "        self.df_gs=pl.read_csv(os.path.join(self.root, 'gas_prices.csv'),\n",
    "                               columns=self.gs_cl ,try_parse_dates=True)\n",
    "\n",
    "        self.df_ec=pl.read_csv(os.path.join(self.root, 'electricity_prices.csv'),\n",
    "                               columns=self.ec_cl, try_parse_dates=True)\n",
    "\n",
    "        self.df_fr=pl.read_csv(os.path.join(self.root, 'forecast_weather.csv'),\n",
    "                               columns=self.fr_cl,try_parse_dates=True)\n",
    "                               \n",
    "        self.df_hs=pl.read_csv(os.path.join(self.root, 'historical_weather.csv'),\n",
    "                               columns=self.hs_cl,try_parse_dates=True)\n",
    "        \n",
    "        self.df_lc=pl.read_csv(os.path.join(self.root, 'weather_station_to_county_mapping.csv'),\n",
    "                               columns=self.lc_cl)\n",
    "        \n",
    "        self.df_tr = self.df_tr.filter(\n",
    "            pl.col('datetime') >= pd.to_datetime('2022-01-01'))\n",
    "        \n",
    "        self.df_tg=self.df_tr.select(self.tg_cl)\n",
    "\n",
    "        self.schema_df_tr = self.df_tr.schema\n",
    "        self.schema_df_cl = self.df_cl.schema\n",
    "        self.schema_df_gs = self.df_gs.schema\n",
    "        self.schema_df_ec = self.df_ec.schema\n",
    "        self.schema_df_fr = self.df_fr.schema\n",
    "        self.schema_df_hs = self.df_hs.schema\n",
    "        self.schema_df_tg = self.df_tg.schema\n",
    "        \n",
    "        self.df_fr=self.df_fr.with_columns(\n",
    "            pl.col('forecast_datetime').cast(pl.Datetime)\n",
    "            )          \n",
    "        \n",
    "        self.df_lc = (self.df_lc.with_columns(\n",
    "            pl.col('latitude').cast(pl.datatypes.Float32),\n",
    "            pl.col('longitude').cast(pl.datatypes.Float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def load_test_data(self,\n",
    "                            df_ts_cl,\n",
    "                            df_ts_gs,\n",
    "                            df_ts_ec,\n",
    "                            df_ts_fr,\n",
    "                            df_ts_hs,\n",
    "                            df_ts_tg\n",
    "                            ):\n",
    "       \n",
    "        df_ts_cl=pl.from_pandas(\n",
    "            df_ts_cl[self.df_cl], schema_overrides = self.schema_df_cl\n",
    "        )\n",
    "\n",
    "        df_ts_gs=pl.from_pandas(\n",
    "            df_ts_gs[self.df_gs], schema_overrides = self.schema_df_gs\n",
    "        )\n",
    "\n",
    "        df_ts_ec=pl.from_pandas(\n",
    "            df_ts_ec[self.df_ec], schema_overrides = self.schema_df_ec\n",
    "        )\n",
    "\n",
    "        df_ts_fr=pl.from_pandas(\n",
    "            df_ts_fr[self.df_fr], schema_overrides = self.schema_df_fr\n",
    "        )\n",
    "\n",
    "        df_ts_hs=pl.from_pandas(\n",
    "            df_ts_hs[self.df_hs], schema_overrides = self.schema_df_hs\n",
    "        )\n",
    "\n",
    "        df_ts_tg=pl.from_pandas(\n",
    "            df_ts_tg[self.tg_cl], schema_overrides = self.schema_df_tg\n",
    "        )\n",
    "\n",
    "\n",
    "        self.df_cl=pl.concat([self.df_cl,df_ts_cl]).unique(\n",
    "            ['date','county','is_bussiness','product_type']\n",
    "            )\n",
    "            \n",
    "        self.df_gs=pl.concat([self.df_gs,df_ts_gs]).unique(\n",
    "            ['forecast_date']\n",
    "            )\n",
    "            \n",
    "        self.df_ec=pl.concat([self.df_ec,df_ts_ec]).unique(\n",
    "            ['forecast_date']\n",
    "            )\n",
    "            \n",
    "        self.df_fr=pl.concat([self.df_fr,df_ts_fr]).unique(\n",
    "            ['forecast_datetime','latitude','longitude','hours_ahead']\n",
    "            )\n",
    "            \n",
    "        self.df_hs=pl.concat([self.df_hs,df_ts_hs]).unique(\n",
    "            ['datetime','latitude','longitude']\n",
    "            )\n",
    "            \n",
    "        self.df_tg=pl.concat([self.df_tg,df_ts_tg]).unique(\n",
    "            ['datetime', 'county', 'is_business', 'product_type', 'is_consumption']\n",
    "            )\n",
    "                \n",
    "    def test_load(self,df_ts):\n",
    "        df_ts=df_ts.reaname(columns={'prediction_datetime':'datetime'})\n",
    "        df_ts=pl.from_pandas(df_ts[self.tr_cl[1:]], schema_overrides=self.schema_df_tr)\n",
    "\n",
    "        return df_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "\n",
    "    def _add_date_feature(self, df_features):\n",
    "        df_features=(\n",
    "            df_features.with_columns(\n",
    "                pl.col('datetime').dt.hour().alias('hour'),\n",
    "                pl.col('datetime').dt.ordinal_day().alias('dayofyear'),\n",
    "                pl.col('datetime').dt.weekday().alias('dayofweek'),\n",
    "                pl.col('datetime').dt.day().alias('day'),\n",
    "                pl.col('datetime').dt.month().alias('month'),\n",
    "                pl.col('datetime').dt.year().alias('year')\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    'county', 'is_business', 'product_type',\n",
    "                    'is_consumption', separator='-'\n",
    "                ).alias('segment')\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col('dayofyear') / 183).sin().alias('SinDayofyear'),\n",
    "                (np.pi * pl.col('dayofyear') / 183).cos().alias('CosDayofyear'),\n",
    "                (np.pi * pl.col('hour') / 12).sin().alias('SinHour'),\n",
    "                (np.pi * pl.col('hour') / 12).cos().alias('CosHour')\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def _add_client_features(self, df_features):\n",
    "        df_cl=self.data.df_cl\n",
    "\n",
    "        df_features=df_features.join(\n",
    "            df_cl.with_columns(\n",
    "                (pl.col('date') + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            how='left', on=['county', 'is_business', 'product_type', 'date']\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def _add_forecast_weather_features(self,df_features):\n",
    "        df_fr=self.data.df_fr\n",
    "        df_lc=self.data.df_lc\n",
    "\n",
    "        df_fr = (\n",
    "            df_fr.rename({'forecast_datetime': 'datetime'})\n",
    "            .filter((pl.col('hours_ahead') >= 22) & (pl.col('hours_ahead') <= 45))\n",
    "                    .drop('hours_ahead')\n",
    "                    .with_columns(\n",
    "                        pl.col('latitude').cast(pl.datatypes.Float32),\n",
    "                        pl.col('longitude').cast(pl.datatypes.Float32)\n",
    "                    )\n",
    "                    .join(\n",
    "                        df_lc, how='left', on=['longitude', 'latitude']\n",
    "                    )\n",
    "                    .drop('longitude', 'latitude')\n",
    "        )\n",
    "                        \n",
    "        df_fr_date=(\n",
    "            df_fr.groupby('datetime').mean().drop('county')\n",
    "        )\n",
    "\n",
    "        df_fr_local=(\n",
    "            df_fr.filter(pl.col('county').is_not_null())\n",
    "            .groupby('county','datetime')\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_fr_date.with_columns(\n",
    "                    pl.col('datetime') + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on='datetime',\n",
    "                how='left',\n",
    "                suffix=f'forecast_{hours_lag}h',\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_fr_local.with_columns(\n",
    "                    pl.col('datetime') + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=['county', 'datetime'],\n",
    "                how='left',\n",
    "                suffix=f'forecast_local_{hours_lag}h',\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_hs = self.data.df_hs\n",
    "        df_lc = self.data.df_lc\n",
    "\n",
    "        df_hs = (\n",
    "            df_hs.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_lc,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_hs_date = (\n",
    "            df_hs.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_hs_local = (\n",
    "            df_hs.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_hs_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_hs_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_hs_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_target_features(self,df_features):\n",
    "        df_tg=self.data.df_tg\n",
    "\n",
    "        df_tg_sum=(\n",
    "            df_tg.groupby(['datetime', 'county', 'is_business', 'is_consumption'])\n",
    "            .sum()\n",
    "            .drop('product_type')\n",
    "        )\n",
    "        \n",
    "        df_tg_county_sum=(\n",
    "            df_tg.groupby(['datetime', 'is_business', 'is_consumption'])\n",
    "            .sum()\n",
    "            .drop('product_type','county')\n",
    "        )\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24]:\n",
    "\n",
    "            df_features=df_features.join(\n",
    "                df_tg.with_columns(\n",
    "                        pl.col('datetime')+pl.duration(hours=hours_lag)\n",
    "                    )\n",
    "                    .rename({'target': f'target_{hours_lag}h'}),\n",
    "                how='left',\n",
    "                on=['county','is_business','product_type',\n",
    "                        'is_consumption','datetime']\n",
    "            )\n",
    "            \n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_tg_sum.with_columns(\n",
    "                    pl.col('datetime') + pl.duration(hours=hours_lag)\n",
    "                    )\n",
    "                    .rename({'target': f'tg_sum_{hours_lag}h'}),\n",
    "                on=['county', 'is_business', 'is_consumption', 'datetime'],\n",
    "                how='left',\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_tg_county_sum.with_columns(\n",
    "                    pl.col('datetime') + pl.duration(hours=hours_lag)\n",
    "                )\n",
    "                .rename({'target': f'tg_county_sum_{hours_lag}h'}),\n",
    "                on=['is_business', 'is_consumption', 'datetime'],\n",
    "                how='left',\n",
    "                suffix=f'tg_county_sum_{hours_lag}h',\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f'target_{hours_lag}h' for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f'target_mean'),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f'target_std'),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            ('target', 24 * 7, 24 * 14),\n",
    "            ('target', 24 * 2, 24 * 9),\n",
    "            ('target', 24 * 3, 24 * 10),\n",
    "            ('target', 24 * 2, 24 * 3),\n",
    "            ('tg_sum', 24 * 2, 24 * 3),\n",
    "            ('tg_sum', 24 * 7, 24 * 14),\n",
    "            ('tg_county_sum', 24 * 2, 24 * 3),\n",
    "            ('tg_county_sum', 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f'{target_prefix}_{lag_nominator}h')\n",
    "                    / (pl.col(f'{target_prefix}_{lag_denomonator}h') + 1e-3)\n",
    "                ).alias(f'{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}')\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _add_addinional_feature(self,df_features):\n",
    "        \n",
    "        df_features.with_columns(pl.col('is_business')\n",
    "                                 .mul(pl.col('eic_count'))\n",
    "                                 .mul(pl.col('installed_capacity'))\n",
    "                                 .alias('best_feature_combo')\n",
    "                                 )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            'date', 'datetime', 'hour', 'dayofyear'\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = ['county','is_business','product_type','is_consumption',\n",
    "                    'segment',\n",
    "        ]\n",
    " \n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index('row_id')\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype('category')\n",
    "\n",
    "        return df_features,cat_cols\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if 'target' in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop('target'),\n",
    "                df_prediction_items.select('target'),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col('datetime').cast(pl.Date).alias('date'),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_date_feature,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_addinional_feature,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features,cat_cols = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeler:\n",
    "    \n",
    "    def __init__(self,data,models,cat_cols):\n",
    "        self.data=data\n",
    "        self.models=models\n",
    "        self.cat_cols=cat_cols\n",
    "\n",
    "    def fit_and_pred(self):\n",
    "        data=self.data\n",
    "        cat_cols=self.cat_cols\n",
    "\n",
    "        data[cat_cols[:-1]]=data[cat_cols[:-1]].astype('int16')\n",
    "        value_counts=data[cat_cols[-1]].value_counts().to_dict()\n",
    "        data[cat_cols[-1]]=data[cat_cols[-1]].map(value_counts)\n",
    "        \n",
    "        df0=data[data['is_business']==0]\n",
    "        df1=data[data['is_business']==1]\n",
    "        X0,X1,y0,y1=df0.drop('target', axis=1), df1.drop('target', axis=1), df0['target'], df1['target']\n",
    "\n",
    "\n",
    "        tmp={'Model':[], 'Dilemma':[] ,'Scores':[], 'Std':[]}\n",
    "        scores=[]\n",
    "        models=self.models\n",
    "        for model in(models):\n",
    "            for dilemma in range(2):       \n",
    "                if dilemma==0:  X,y=X0,y0 \n",
    "                if dilemma==1:  X,y=X1,y1\n",
    "                tmp['Model'].append(model.__class__.__name__)\n",
    "                tmp['Dilemma'].append(dilemma)\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    x_tr, x_ts = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_tr, y_ts = y.iloc[train_index], y.iloc[test_index]\n",
    "                \n",
    "                    model.fit(x_tr,y_tr)\n",
    "                    y_pr=model.predict(x_ts)\n",
    "                    score=mean_squared_error(y_ts,y_pr,squared=False)\n",
    "                    scores.append(score)\n",
    "                tmp['Scores'].append(np.round(np.mean(scores),3))\n",
    "                tmp['Std'].append(np.round(np.std(scores),3))\n",
    "        \n",
    "        return tmp\n",
    "    \n",
    "    def reporting(self,tmp):\n",
    "        result=pd.DataFrame(tmp)\n",
    "        return result\n",
    "    \n",
    "    def run(self):\n",
    "        tmp = self.fit_and_pred()\n",
    "        result =self.reporting(tmp)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=DataLoader()\n",
    "enhance_data=DataPreprocessor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the data fell good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_features,cat_cols = enhance_data.generate_features(data.df_tr)\n",
    "df_tr_features = df_tr_features[df_tr_features['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estonian_holidays = holidays.country_holidays('EE', years=range(2021, 2026))\n",
    "estonian_holidays = list(estonian_holidays.keys())\n",
    "\n",
    "def add_holidays_as_binary_features(df):\n",
    "    df['country_holiday'] = df.apply(lambda row: (datetime.date(row['year'], row['month'], row['day']) in estonian_holidays) * 1, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_tr_features = add_holidays_as_binary_features(df_tr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Succsess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dilemma</th>\n",
       "      <th>Scores</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0</td>\n",
       "      <td>71.221</td>\n",
       "      <td>1.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>1</td>\n",
       "      <td>101.514</td>\n",
       "      <td>30.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>0</td>\n",
       "      <td>88.794</td>\n",
       "      <td>30.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>1</td>\n",
       "      <td>96.369</td>\n",
       "      <td>29.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>0</td>\n",
       "      <td>92.992</td>\n",
       "      <td>27.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>1</td>\n",
       "      <td>103.844</td>\n",
       "      <td>34.814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Dilemma   Scores     Std\n",
       "0       XGBRegressor        0   71.221   1.545\n",
       "1       XGBRegressor        1  101.514  30.337\n",
       "2  CatBoostRegressor        0   88.794  30.621\n",
       "3  CatBoostRegressor        1   96.369  29.597\n",
       "4      LGBMRegressor        0   92.992  27.324\n",
       "5      LGBMRegressor        1  103.844  34.814"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models=[xgb.XGBRegressor(),cbt.CatBoostRegressor(verbose=0),lgbm.LGBMRegressor(verbose=-1)]\n",
    "hope=Modeler(df_tr_features,models,cat_cols=cat_cols)\n",
    "result=hope.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
